{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# Use tensorflow 1 behavior to match the Universal Sentence Encoder\n",
    "# examples (https://tfhub.dev/google/universal-sentence-encoder/2).\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.ontology_parsing.data_loading import get_all_concept_file_paths, get_graphs_from_files\n",
    "from config import ONTOLOGY_CORE_DIR\n",
    "\n",
    "# reading ontology\n",
    "files = get_all_concept_file_paths(ONTOLOGY_CORE_DIR)\n",
    "graphs = get_graphs_from_files(files)\n",
    "\n",
    "# To be defined in config - index column names and their uris\n",
    "pred_uri_to_idx_colname = {\n",
    "    'http://www.w3.org/2004/02/skos/core#prefLabel': 'label',\n",
    "    'http://www.w3.org/2004/02/skos/core#closeMatch': 'closeMatch',\n",
    "    'http://www.w3.org/2004/02/skos/core#related': 'related',\n",
    "    'http://www.w3.org/2004/02/skos/core#broader': 'broader'\n",
    "}\n",
    "pred_uri_to_idx_colname\n",
    "\n",
    "\n",
    "from source.ontology_parsing.graph_utils import get_uri_to_colname_dict_from_ontology\n",
    "\n",
    "# or derived from the ontology automatically\n",
    "pred_uri_to_idx_colname = get_uri_to_colname_dict_from_ontology(graphs)\n",
    "pred_uri_to_idx_colname\n",
    "\n",
    "from source.es_index.IndexBaseline import IndexBaseline\n",
    "\n",
    "index_builder = IndexBaseline(pred_uri_to_idx_colname, graphs, include_concept_type=True)\n",
    "\n",
    "rows = index_builder.build_rows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTOR EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define template\n",
    "template = \"\"\"\n",
    "{\n",
    "    \"settings\": {\n",
    "    \"number_of_shards\": 2,\n",
    "    \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"true\",\n",
    "        \"_source\": {\n",
    "        \"enabled\": \"true\"\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"label_vector\":{\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 20\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"closeMatch\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"related\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"broader\": {\n",
    "                \"type\": \"keyword\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ontology_vector'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to connect to\n",
    "\n",
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print('Yay Connected')\n",
    "    else:\n",
    "        print('Awww it could not connect!')\n",
    "    return _es\n",
    "\n",
    "import requests \n",
    "import json\n",
    "\n",
    "query = json.loads(template)\n",
    "response = requests.put('http://localhost:9200/ontology_vector',\n",
    "                       json=query)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"posts\"\n",
    "SEARCH_SIZE = 5\n",
    "GPU_LIMIT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_batch(docs):\n",
    "    labels = [doc[\"label\"] for doc in docs]\n",
    "    label_vectors = embed_text(titles)\n",
    "\n",
    "    requests = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        request = doc\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = INDEX_NAME\n",
    "        request[\"label_vector\"] = label_vectors[i]\n",
    "        requests.append(request)\n",
    "    bulk(client, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(text):\n",
    "    vectors = session.run(embeddings, feed_dict={text_ph: text})\n",
    "    return [vector.tolist() for vector in vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pre-trained embeddings from tensorflow hub...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbf in position 156: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDownloading pre-trained embeddings from tensorflow hub...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m embed \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39;49mModule(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://tfhub.dev/google/universal-sentence-encoder/4\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m text_ph \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mplaceholder(tf\u001b[39m.\u001b[39mstring)\n\u001b[0;32m      4\u001b[0m embeddings \u001b[39m=\u001b[39m embed(text_ph)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\module.py:157\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Constructs a Module to be used in the current graph.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \n\u001b[0;32m    125\u001b[0m \u001b[39mThis creates the module `state-graph` under an unused variable_scope based\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39m  tf.errors.NotFoundError: if the requested graph contains unknown ops.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m--> 157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_spec \u001b[39m=\u001b[39m as_module_spec(spec)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainable \u001b[39m=\u001b[39m trainable\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tags \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(tags \u001b[39mor\u001b[39;00m [])\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\module.py:30\u001b[0m, in \u001b[0;36mas_module_spec\u001b[1;34m(spec)\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[39mreturn\u001b[39;00m spec\n\u001b[0;32m     29\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(spec, \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 30\u001b[0m   \u001b[39mreturn\u001b[39;00m load_module_spec(spec)\n\u001b[0;32m     31\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown module spec type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(spec))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\module.py:65\u001b[0m, in \u001b[0;36mload_module_spec\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Loads a ModuleSpec from a TF Hub service or the filesystem.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[39mWarning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m  tf.errors.OpError: on file handling exceptions.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m path \u001b[39m=\u001b[39m registry\u001b[39m.\u001b[39mresolver(path)\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39;49mloader(path)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\registry.py:51\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfor\u001b[39;00m impl \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impls):\n\u001b[0;32m     50\u001b[0m   \u001b[39mif\u001b[39;00m impl\u001b[39m.\u001b[39mis_supported(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     fails\u001b[39m.\u001b[39mappend(\u001b[39mtype\u001b[39m(impl)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\native_module.py:139\u001b[0m, in \u001b[0;36mLoader.__call__\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m--> 139\u001b[0m   module_def_proto \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module_def_proto(path)\n\u001b[0;32m    141\u001b[0m   \u001b[39mif\u001b[39;00m module_def_proto\u001b[39m.\u001b[39mformat \u001b[39m!=\u001b[39m module_def_pb2\u001b[39m.\u001b[39mModuleDef\u001b[39m.\u001b[39mFORMAT_V3:\n\u001b[0;32m    142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnsupported module def format: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    143\u001b[0m                      module_def_proto\u001b[39m.\u001b[39mformat)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow_hub\\native_module.py:130\u001b[0m, in \u001b[0;36mLoader._get_module_def_proto\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    128\u001b[0m module_def_proto \u001b[39m=\u001b[39m module_def_pb2\u001b[39m.\u001b[39mModuleDef()\n\u001b[0;32m    129\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mOpen(module_def_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 130\u001b[0m   module_def_proto\u001b[39m.\u001b[39mParseFromString(f\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m module_def_proto\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:116\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, n\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    105\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[39m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preread_check()\n\u001b[0;32m    117\u001b[0m   \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m    118\u001b[0m     length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\sdp\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:77\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_check_passed:\n\u001b[0;32m     75\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mPermissionDeniedError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     76\u001b[0m                                      \u001b[39m\"\u001b[39m\u001b[39mFile isn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt open for reading\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_buf \u001b[39m=\u001b[39m _pywrap_file_io\u001b[39m.\u001b[39;49mBufferedInputStream(\n\u001b[0;32m     78\u001b[0m     compat\u001b[39m.\u001b[39;49mpath_to_str(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name), \u001b[39m1024\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39m512\u001b[39;49m)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbf in position 156: invalid start byte"
     ]
    }
   ],
   "source": [
    "print(\"Downloading pre-trained embeddings from tensorflow hub...\")\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "text_ph = tf.placeholder(tf.string)\n",
    "embeddings = embed(text_ph)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating tensorflow session...\")\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = GPU_LIMIT\n",
    "session = tf.Session(config=config)\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.tables_initializer())\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_batch(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_query(query):\n",
    "\n",
    "    embedding_start = time.time()\n",
    "    query_vector = embed_text([query])[0]\n",
    "    embedding_time = time.time() - embedding_start\n",
    "\n",
    "    script_query = {\n",
    "        \"script_score\": {\n",
    "            \"query\": {\"match_all\": {}},\n",
    "            \"script\": {\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0\",\n",
    "                \"params\": {\"query_vector\": query_vector}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_start = time.time()\n",
    "    response = client.search(\n",
    "        index=INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\": {\"includes\": [\"title\", \"body\"]}\n",
    "        }\n",
    "    )\n",
    "    search_time = time.time() - search_start\n",
    "\n",
    "    print()\n",
    "    print(\"{} total hits.\".format(response[\"hits\"][\"total\"][\"value\"]))\n",
    "    print(\"embedding time: {:.2f} ms\".format(embedding_time * 1000))\n",
    "    print(\"search time: {:.2f} ms\".format(search_time * 1000))\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        print(\"id: {}, score: {}\".format(hit[\"_id\"], hit[\"_score\"]))\n",
    "        print(hit[\"_source\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT ANALYZER PROPERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{\n",
    "    \"settings\": {\n",
    "    \"number_of_shards\": 2,\n",
    "    \"number_of_replicas\": 1\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"true\",\n",
    "        \"_source\": {\n",
    "        \"enabled\": \"true\"\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\":\"english\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"closeMatch\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"related\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\":\"english\"\n",
    "            },\n",
    "            \"broader\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\":\"english\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ontology_analyzer'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = json.loads(template)\n",
    "response = requests.put('http://localhost:9200/ontology_analyzer',\n",
    "                       json=query)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_load(rows, client, index_name):\n",
    "    requests = []\n",
    "    for i, row in enumerate(rows):\n",
    "        request = row\n",
    "        request[\"_op_type\"] = \"index\"\n",
    "        request[\"_index\"] = index_name\n",
    "        requests.append(request)\n",
    "    bulk(client, requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print('Yay Connected')\n",
    "    else:\n",
    "        print('Awww it could not connect!')\n",
    "    return _es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay Connected\n"
     ]
    }
   ],
   "source": [
    "client = connect_elasticsearch()\n",
    "bulk_load(rows, client)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEYWORD TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"closeMatch\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"related\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"broader\": {\n",
    "                \"type\": \"keyword\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ontology_keyword'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = json.loads(template)\n",
    "response = requests.put('http://localhost:9200/ontology_keyword',\n",
    "                       json=query)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay Connected\n"
     ]
    }
   ],
   "source": [
    "client = connect_elasticsearch()\n",
    "bulk_load(rows, client, 'ontology_keyword')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "        \"similarity\": {\n",
    "            \"my_similarity\": {\n",
    "                \"type\": \"DFR\",\n",
    "                \"basic_model\": \"g\",\n",
    "                \"after_effect\": \"l\",\n",
    "                \"normalization\": \"h2\",\n",
    "                \"normalization.h2.c\": \"3.0\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"my_similarity\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"closeMatch\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"related\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"my_similarity\"\n",
    "            },\n",
    "            \"broader\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"my_similarity\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ontology_similarity'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = json.loads(template)\n",
    "response = requests.put('http://localhost:9200/ontology_similarity',\n",
    "                       json=query)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay Connected\n"
     ]
    }
   ],
   "source": [
    "client = connect_elasticsearch()\n",
    "bulk_load(rows, client, 'ontology_similarity')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMILARITY TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"number_of_shards\": 1,\n",
    "    \"similarity\": {\n",
    "      \"scripted_tfidf\": {\n",
    "        \"type\": \"scripted\",\n",
    "        \"script\": {\n",
    "          \"source\": \"double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"label\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"scripted_tfidf\"\n",
    "            },\n",
    "            \"type\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"closeMatch\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"related\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"scripted_tfidf\"\n",
    "            },\n",
    "            \"broader\": {\n",
    "                \"type\": \"keyword\",\n",
    "                \"similarity\" : \"scripted_tfidf\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True,\n",
       " 'shards_acknowledged': True,\n",
       " 'index': 'ontology_similarity_tfidf'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = json.loads(template)\n",
    "response = requests.put('http://localhost:9200/ontology_similarity_tfidf',\n",
    "                       json=query)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay Connected\n"
     ]
    }
   ],
   "source": [
    "client = connect_elasticsearch()\n",
    "bulk_load(rows, client, 'ontology_similarity_tfidf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b5960174be4c5aac43f8933afc24dcaceb58b272c5d28c20878db9a6a7564ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
